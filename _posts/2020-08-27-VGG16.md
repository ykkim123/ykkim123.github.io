---
layout: post
title: "VGG16"
date: "2020-08-27"
description: "This post explains how to implement VGG16 using PyTorch-from adjusting model for the data to visualizing the output from intermediate layer using dogsandcats dataset."
category: 
  - featured
# tags will also be used as html meta keywords.
tags:
  - Image Recognition
show_meta: true
comments: true
mathjax: true
gistembed: true
published: true
noindex: false
nofollow: false
---

# Load data

The procedure of loading *dogsandcats* dataset is the same as that of vanilla CNN(click [here](https://github.com/ykkim123/Data_Science/blob/master/Vanilla_CNN/Vanilla_CNN-dogsandcats1.ipynb) for more details).

<code data-gist-id="aa03324b1c7d86bb83fbd2c7d0267d43" data-gist-file="VGG16-dogsandcats.py" data-gist-line="36,41-45,50-52"></code>

<br>

# Adjust VGG16

In this post, we are going to classify the data using transfer learning. A VGG16 model, which is a pre-trained model on *ImageNet* data, will be used to classify images.

![structure]({{ site.urlimg }}/VGG16/structure.png)

<br>

In order to take an advantage of pre-training, we are going to freeze layers of *features* and train the data only for layers belong to *classifier*. Also, since the model was trained to predict images of 1,000 categories, we should adjust the number of output channels in the last linear layer so that the algorithm classifies images into either dog or cat.

<code data-gist-id="aa03324b1c7d86bb83fbd2c7d0267d43" data-gist-file="VGG16-dogsandcats.py" data-gist-line="62,68-72"></code>

The procedure of training the model is similar to that of vanilla CNN. However, note that we are going to use loss based on cross entropy, not negative log-likelihood.

<code data-gist-id="aa03324b1c7d86bb83fbd2c7d0267d43" data-gist-file="VGG16-dogsandcats.py" data-gist-line="78-107, 112-114,119-130"></code>

You can improve the model in some ways. First, let's change *p* of dropout layers in *classifier* to 0.2, whose default value is 0.5.

<code data-gist-id="aa03324b1c7d86bb83fbd2c7d0267d43" data-gist-file="VGG16-dogsandcats.py" data-gist-line="156-158"></code>

Or you can do data augmentation to improve the model. For example, run the code below to flip the image horizontally and rotate it.

<code data-gist-id="aa03324b1c7d86bb83fbd2c7d0267d43" data-gist-file="VGG16-dogsandcats.py" data-gist-line="218-221"></code>

<br>

# Calculating pre-convoluted features

When we use the result from pre-trained VGG16, parameters are not updated for layers in *features*. This implies that we can reduce computation time by reusing the result of *features* layers.  

<code data-gist-id="aa03324b1c7d86bb83fbd2c7d0267d43" data-gist-file="VGG16-dogsandcats.py" data-gist-line="276-288,293-295"></code>

Note that the size of *conv_feat_train* is (23000 X 512 X 7 X 7). Each number indicates

- 23000: the number of images

- 512: the number of output channels

- 7 X 7 is an output of

  1. 224 X 224: original data
  2. 112 X 112: after max pooling1 with stride=2
  3. 56 X 56: after max pooling2 with stride=2
  4. 28 X 28: after max pooling3 with stride=2
  5. 14 X 14: after max pooling4 with stride=2
  6. 7 X 7: after max pooling5 with stride=2

  - Convolutional layers do not change size, since it satisfies

    <br>
    $$
    O=\frac{W-K+2P}{S}+1
    $$
    <br>
    
    where 
    $$
    O,W,K,P,S
    $$
     indicates the size of output, input, filter, padding, and stride respectively

<br>

Then, using *My_dataset* class, we can create *DataLoader* for convolution features

<code data-gist-id="aa03324b1c7d86bb83fbd2c7d0267d43" data-gist-file="VGG16-dogsandcats.py" data-gist-line="308-317,322-324,329-331"></code>

and train only for *features* layers, which takes much less time.

<code data-gist-id="aa03324b1c7d86bb83fbd2c7d0267d43" data-gist-file="VGG16-dogsandcats.py" data-gist-line="337-366,371-382"></code>

<br>

![intermediate_layer_output2]({{ site.urlimg }}/VGG16/loss.png)

![intermediate_layer_output2]({{ site.urlimg }}/VGG16/accuracy.png)

# Visualize output from intermediate layers

Sometimes, visualizing output from intermediate layer is helpful in understanding how do each layer work. An example image that will be taken as an input looks like:

![intermediate_layer_image]({{ site.urlimg }}/VGG16/intermediate_layer_image.png)

The *LayerActivation* class in the code below extracts an output from intermediate layer.

<code data-gist-id="aa03324b1c7d86bb83fbd2c7d0267d43" data-gist-file="VGG16-dogsandcats.py" data-gist-line="440-450"></code>

- *self.hook* extracts output from *model* and *layer_num*, by calling *register_forward_hook* with *hook_fn*
- *hook_fn* extracts output from *module* and *input*
- *remove* removes *hook_fn*

<br>

And output of the first and last convolutional layer can be shown as below:

![intermediate_layer_output1]({{ site.urlimg }}/VGG16/intermediate_layer_output0.png)

![intermediate_layer_output2]({{ site.urlimg }}/VGG16/intermediate_layer_output1.png)

You can see that the first convolutional layer detects line and edge, while the last convolutional layer tends to learn more high-level features. This kind of work may help to figure out why the model does not work well. 

Also, we can visualize weight from intermediate layer in a similar way. For example, we can visualize first 30 filters of the first convolutional layer:

![intermediate_layer_output2]({{ site.urlimg }}/VGG16/intermediate_layer_weight.png)

<br>

And some other examples using VGG16 can be found [here](https://github.com/ykkim123/Data_Science/tree/master/VGG16).

<br>

# References

- [https://github.com/svishnu88/DLwithPyTorch](https://github.com/svishnu88/DLwithPyTorch)