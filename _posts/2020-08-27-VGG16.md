---
layout: post
title: "VGG16"
date: "2020-08-27"
description: "This post explains how to implement VGG16 using PyTorch-from adjusting model for the data to visualizing the output from intermediate layer using dogsandcats dataset."
category: 
  - featured
# tags will also be used as html meta keywords.
tags:
  - Image Recognition
show_meta: true
comments: true
mathjax: true
gistembed: true
published: true
noindex: false
nofollow: false
---

# Load data

Loading *dogsandcats* data can be done in a similar way to vanilla CNN(click [here](https://github.com/ykkim123/Data_Science/blob/master/Vanilla_CNN/Vanilla_CNN-dogsandcats1.ipynb) for more details).

<code data-gist-id="722060d632bcf1d1d26bd96510325623" data-gist-file="VGG16-dogsandcats.py" data-gist-line="28-35,41-42"></code>

<br>

# Adjust VGG16

In this post, we are going to classify the data using transfer learning. That is, a VGG16 model, which is pre-trained on *ImageNet* data, will be used to classify images.

The code below loads a VGG16 model and prints out structure of the model. You can see that it is a complicated version of vanilla CNN.

<code data-gist-id="722060d632bcf1d1d26bd96510325623" data-gist-file="Convolutional-Neural-Network.py" data-gist-line="52-53,59"></code>

![structure]({{ site.urlimg }}/VGG16/structure.png)

In order to use pre-trained result of VGG16, we are going to freeze layers of *features* and train the data only for layers of *classifier*. Also, since the model was trained to predict images of 1,000 categories, we should adjust this so that the algorithm classifies images into either dog or cat.

<code data-gist-id="722060d632bcf1d1d26bd96510325623" data-gist-file="VGG16-dogsandcats.py" data-gist-line="65-66,68"></code>

The procedure for training the model is similar to that of vanilla CNN. However, note that we are going to use loss based on cross entropy, not negative log-likelihood.

<code data-gist-id="722060d632bcf1d1d26bd96510325623" data-gist-file="VGG16-dogsandcats.py" data-gist-line="74-105,111-125"></code>

<br>

You can improve the model in certain ways. First, change *p* of dropout layers in *classifier* to 0.2, whose default value is 0.5.

<code data-gist-id="722060d632bcf1d1d26bd96510325623" data-gist-file="VGG16-dogsandcats.py" data-gist-line="151-155"></code>

Or you can do data augmentation to improve the model. For example, run the code below to flip the image horizontally and rotate it.

<code data-gist-id="1245d8e68d078207fd2bad1a061e6245" data-gist-file="VGG16-dogsandcats.py" data-gist-line="215-219"></code>

<br>

# Calculate pre-convoluted features

When we use the result from pre-trained VGG16, parameters are not updated for layers in *classifier*. This implies that we can reduce computation time by reusing the result from *classifier* layers.  

<code data-gist-id="722060d632bcf1d1d26bd96510325623" data-gist-file="VGG16-dogsandcats.py" data-gist-line="271-284,290-291"></code>

Note that the size of *conv_feat_train* is (23000 X 512 X 7 X 7). Each number indicates

- 23000: the number of images

- 512: the number of output channels

- 7 X 7 is an output of

  1. 224 X 224: original data
  2. 112 X 112: after max pooling1 with stride=2
  3. 56 X 56: after max pooling2 with stride=2
  4. 28 X 28: after max pooling3 with stride=2
  5. 14 X 14: after max pooling4 with stride=2
  6. 7 X 7: after max pooling5 with stride=2

  - convolutional layers do not change the size, since 

    <br>
    $$
    O=\frac{W-K+2P}{S}+1
    $$
    where 
    $$
    O,W,K,P,S
    $$
     indicates the size of output, input, filter, padding, and stride respectively

<br>

Then, using *My_dataset* class, we can create *DataLoader* for convolution features

<code data-gist-id="722060d632bcf1d1d26bd96510325623" data-gist-file="VGG16-dogsandcats.py" data-gist-line="304-315,321-322,328-329"></code>

and train only for feature layers in similar way

<code data-gist-id="722060d632bcf1d1d26bd96510325623" data-gist-file="VGG16-dogsandcats.py" data-gist-line="335-364"></code>

which takes much less time.

<br>

# Visualize output from intermediate layers

Sometimes, visualizing output from intermediate layers is helpful in understanding how do each layer work. An example image that will be taken as an input looks like:

![intermediate_layer_image]({{ site.urlimg }}/VGG16/intermediate_layer_image.png)

The *LayerActivation* class in the code below extracts an output from intermediate layer.

<code data-gist-id="722060d632bcf1d1d26bd96510325623" data-gist-file="VGG16-dogsandcats.py" data-gist-line="436-446"></code>

- *self.hook* extracts output from *model* and *layer_num*, by calling *register_forward_hook* with *hook_fn*
- *hook_fn* extracts output from *module* and *input*
- *remove* removes *hook_fn*

<br>

And output of the first and last convolutional layer can be shown as below:

![intermediate_layer_output1]({{ site.urlimg }}/VGG16/intermediate_layer_output2.png)

![intermediate_layer_output2]({{ site.urlimg }}/VGG16/intermediate_layer_output2.png)

You can see that the first convolutional layer detects line and edge, while the last convolutional layer tends to learn more high-level features. This kind of work may help to figure out why the model does not work well. 

Also, we can visualize weight from intermediate layer in a similar way. For example, we can visualize first 30 filters of the first convolutional layer. 

![intermediate_layer_output2]({{ site.urlimg }}/VGG16/intermediate_layer_weight.png)

<br>

And some other examples using VGG16 can be found [here](https://github.com/ykkim123/Data_Science/tree/master/Vanilla_CNN).

<br>

# References

- [https://github.com/svishnu88/DLwithPyTorch](https://github.com/svishnu88/DLwithPyTorch)